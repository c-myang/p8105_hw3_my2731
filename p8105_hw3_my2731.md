P8105 Homework 3
================
October 15th, 2022

## Problem 1: Instacart Data

We will be working with the “The Instacart Online Grocery Shopping
Dataset 2017”.

### Data overview and description

``` r
data("instacart")
instacart 
```

    ## # A tibble: 1,384,617 × 15
    ##    order_id product_id add_to_…¹ reord…² user_id eval_…³ order…⁴ order…⁵ order…⁶
    ##       <int>      <int>     <int>   <int>   <int> <chr>     <int>   <int>   <int>
    ##  1        1      49302         1       1  112108 train         4       4      10
    ##  2        1      11109         2       1  112108 train         4       4      10
    ##  3        1      10246         3       0  112108 train         4       4      10
    ##  4        1      49683         4       0  112108 train         4       4      10
    ##  5        1      43633         5       1  112108 train         4       4      10
    ##  6        1      13176         6       0  112108 train         4       4      10
    ##  7        1      47209         7       0  112108 train         4       4      10
    ##  8        1      22035         8       1  112108 train         4       4      10
    ##  9       36      39612         1       0   79431 train        23       6      18
    ## 10       36      19660         2       1   79431 train        23       6      18
    ## # … with 1,384,607 more rows, 6 more variables: days_since_prior_order <int>,
    ## #   product_name <chr>, aisle_id <int>, department_id <int>, aisle <chr>,
    ## #   department <chr>, and abbreviated variable names ¹​add_to_cart_order,
    ## #   ²​reordered, ³​eval_set, ⁴​order_number, ⁵​order_dow, ⁶​order_hour_of_day

The dataset contains information about online grocery orders made
through Instacart. There 1,384,617 observations of 15 variables, where
each row in the dataset is a product from an order. Each order is
associated with a unique order ID number and order ID. The variables
`order_hour_of_day` and `order_dow` describe the time (hour of day and
day of the week) orders were made. Each row also contains information
about each product, such as `product_name`, `aisle`, and `department` of
the product. The variable `reordered` indicates whether a product has
been purchased by a user in the past, and `add_to_cart_order` indicates
the order an item was placed in the cart.

For example, for `order_id == 1`, we can see the order was made at 10AM
on a Thursday, 4 out of the 8 products are reorders, and that most of
the products came from the produce and dairy eggs department.

## Data exploration

Now, we want to conduct some basic EDA and answer some questions about
the `instacart` data.

1.  Number of aisles

Using `group_by`, and `summarise`, we can determine the number of
aisles, and the most popular aisles customers order from in the dataset.

``` r
aisles = instacart %>% 
  group_by(aisle_id, aisle) %>% 
  summarise(
    n_obs = n()) %>% 
  arrange(desc(n_obs))

aisles
```

    ## # A tibble: 134 × 3
    ## # Groups:   aisle_id [134]
    ##    aisle_id aisle                          n_obs
    ##       <int> <chr>                          <int>
    ##  1       83 fresh vegetables              150609
    ##  2       24 fresh fruits                  150473
    ##  3      123 packaged vegetables fruits     78493
    ##  4      120 yogurt                         55240
    ##  5       21 packaged cheese                41699
    ##  6      115 water seltzer sparkling water  36617
    ##  7       84 milk                           32644
    ##  8      107 chips pretzels                 31269
    ##  9       91 soy lactosefree                26240
    ## 10      112 bread                          23635
    ## # … with 124 more rows

There are 134 different aisles. The most popular aisles customers order
from are fresh vegetables, fresh fruits, and packaged vegetables fruits,
respectively.

2.  Plotting number of items ordered in each aisle

``` r
plot_aisles = aisles %>% 
  filter(n_obs >= 10000) %>% 
  ggplot(aes(x = reorder(aisle, n_obs), y = n_obs)) +
  geom_bar(stat = "identity", fill = "forestgreen") + 
  coord_flip() +
  labs(
    title = "Number of Instacart items ordered by aisle",
    x = "Aisle",
    y = "Number of items ordered",
    caption = "Data from The Instacart Online Grocery Shopping Dataset 2017 in the p8105.datasets package"
  )

plot_aisles
```

![](p8105_hw3_my2731_files/figure-gfm/plot_aisles-1.png)<!-- -->

Through the bar plot, we can confirm that for aisles with over 10,000
items ordered, the most popular aisles are fresh fruits and vegetables,
and the least popular aisles are butter, oils and vinegars, and dry
pasta.

3.  Popular items

Make a table showing the three most popular items in each of the aisles
“baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
Include the number of times each item is ordered in your table.

``` r
pop_items = instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarise(n_obs = n()) %>% 
  arrange(desc(n_obs),.by_group = TRUE) %>% 
  slice(1) %>% 
  arrange(desc(n_obs)) %>% 
  rename("Aisle" = aisle,
         "Product name" = product_name,
         "# of orders" = n_obs)

knitr::kable(pop_items)
```

| Aisle                      | Product name                                    | \# of orders |
|:---------------------------|:------------------------------------------------|-------------:|
| packaged vegetables fruits | Organic Baby Spinach                            |         3324 |
| baking ingredients         | Light Brown Sugar                               |          157 |
| dog food care              | Organix Grain Free Chicken & Vegetable Dog Food |           14 |

4.  Mean ordering times

Using `pivot_wider`, we can create a table showing the mean hour (scaled
to a 24-hour day) at which Pink Lady Apples and Coffee Ice Cream are
ordered on each day of the week.

``` r
instacart %>% 
  filter(product_name == c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(avg_hour = round(mean(order_hour_of_day), 2)) %>% 
  pivot_wider(product_name, 
              names_from = order_dow,
              values_from = avg_hour) %>% 
  knitr::kable(col.names = c("Product name", "Sunday", "Monday", "Tuesday", "Wednesday",
                             "Thursday", "Friday", "Saturday"))
```

| Product name     | Sunday | Monday | Tuesday | Wednesday | Thursday | Friday | Saturday |
|:-----------------|-------:|-------:|--------:|----------:|---------:|-------:|---------:|
| Coffee Ice Cream |  13.22 |  15.00 |   15.33 |     15.40 |    15.17 |  10.33 |    12.35 |
| Pink Lady Apples |  12.25 |  11.68 |   12.00 |     13.94 |    11.91 |  13.87 |    11.56 |

Based on the table, Coffee Ice Cream is usually ordered in the early
afternoon for every day of the week while Pink Lady Apples range from
being ordered after 11am and after 2pm.

## Problem 2: Accelerometer Data

### Clean and load data

In this problem, we will load, clean, and tidy the `accel_data.csv`
dataset, which contains five weeks of accelerometer data collected on a
63 year-old male with BMI 25, admitted to the Advanced Cardiac Care
Center of Columbia University Medical Center and diagnosed with
congestive heart failure (CHF).

We will apply snake case to all the variables, categorize and re-level
the `day` variable according to the days of the week, and add a logical
`weekend` variable that is `TRUE` if `day == c("Saturday", "Sunday")`
and `FALSE` otherwise.

``` r
accel_data =
  read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    weekend = ifelse(day == "Saturday" | day == "Sunday", TRUE, FALSE),
    day = factor(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", 
                                 "Thursday", "Friday", "Saturday"))) %>% 
  relocate(weekend, .after = day)

accel_data
```

    ## # A tibble: 35 × 1,444
    ##     week day_id day      weekend activ…¹ activ…² activ…³ activ…⁴ activ…⁵ activ…⁶
    ##    <dbl>  <dbl> <fct>    <lgl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>
    ##  1     1      1 Friday   FALSE      88.4    82.2    64.4    70.0    75.0    66.3
    ##  2     1      2 Monday   FALSE       1       1       1       1       1       1  
    ##  3     1      3 Saturday TRUE        1       1       1       1       1       1  
    ##  4     1      4 Sunday   TRUE        1       1       1       1       1       1  
    ##  5     1      5 Thursday FALSE      47.4    48.8    46.9    35.8    49.0    44.8
    ##  6     1      6 Tuesday  FALSE      64.8    59.5    73.7    45.7    42.4    58.4
    ##  7     1      7 Wednesd… FALSE      71.1   103.     68.5    45.4    37.8    18.3
    ##  8     2      8 Friday   FALSE     675     542    1010     779     509     106  
    ##  9     2      9 Monday   FALSE     291     335     393     335     263     675  
    ## 10     2     10 Saturday TRUE       64      11       1       1       1       1  
    ## # … with 25 more rows, 1,434 more variables: activity_7 <dbl>,
    ## #   activity_8 <dbl>, activity_9 <dbl>, activity_10 <dbl>, activity_11 <dbl>,
    ## #   activity_12 <dbl>, activity_13 <dbl>, activity_14 <dbl>, activity_15 <dbl>,
    ## #   activity_16 <dbl>, activity_17 <dbl>, activity_18 <dbl>, activity_19 <dbl>,
    ## #   activity_20 <dbl>, activity_21 <dbl>, activity_22 <dbl>, activity_23 <dbl>,
    ## #   activity_24 <dbl>, activity_25 <dbl>, activity_26 <dbl>, activity_27 <dbl>,
    ## #   activity_28 <dbl>, activity_29 <dbl>, activity_30 <dbl>, …

The dataset contains 35 observations of 1444 variables, where each row
describes the activity counts for all minutes of each day. The weekday
of observation is indicated by `day`, and has a unique `day_id`. `week`
indicates the week of observation, and the `weekend` indicates whether
observation falls on a weekend. The variables `activity_*` are the
activity counts for all 1440 minutes of a 24-hour day, starting at
midnight.

### Aggregate across minutes

Using `rowSums` and `across` within the `mutate` function, we can
compute the sum across minutes for each day to get the total activity
over the day.

``` r
accel_data = accel_data %>%
  mutate(total_activity = rowSums(across(activity_1:activity_1440))) %>% 
  relocate(total_activity, .after = weekend) 

accel_data %>% 
  select(week:total_activity) %>% 
  rename("Week" = week,
         "Day ID" = day_id,
         "Day" = day,
         "Weekend" = weekend,
         "Total Activity" = total_activity) %>% 
  knitr::kable()
```

| Week | Day ID | Day       | Weekend | Total Activity |
|-----:|-------:|:----------|:--------|---------------:|
|    1 |      1 | Friday    | FALSE   |      480542.62 |
|    1 |      2 | Monday    | FALSE   |       78828.07 |
|    1 |      3 | Saturday  | TRUE    |      376254.00 |
|    1 |      4 | Sunday    | TRUE    |      631105.00 |
|    1 |      5 | Thursday  | FALSE   |      355923.64 |
|    1 |      6 | Tuesday   | FALSE   |      307094.24 |
|    1 |      7 | Wednesday | FALSE   |      340115.01 |
|    2 |      8 | Friday    | FALSE   |      568839.00 |
|    2 |      9 | Monday    | FALSE   |      295431.00 |
|    2 |     10 | Saturday  | TRUE    |      607175.00 |
|    2 |     11 | Sunday    | TRUE    |      422018.00 |
|    2 |     12 | Thursday  | FALSE   |      474048.00 |
|    2 |     13 | Tuesday   | FALSE   |      423245.00 |
|    2 |     14 | Wednesday | FALSE   |      440962.00 |
|    3 |     15 | Friday    | FALSE   |      467420.00 |
|    3 |     16 | Monday    | FALSE   |      685910.00 |
|    3 |     17 | Saturday  | TRUE    |      382928.00 |
|    3 |     18 | Sunday    | TRUE    |      467052.00 |
|    3 |     19 | Thursday  | FALSE   |      371230.00 |
|    3 |     20 | Tuesday   | FALSE   |      381507.00 |
|    3 |     21 | Wednesday | FALSE   |      468869.00 |
|    4 |     22 | Friday    | FALSE   |      154049.00 |
|    4 |     23 | Monday    | FALSE   |      409450.00 |
|    4 |     24 | Saturday  | TRUE    |        1440.00 |
|    4 |     25 | Sunday    | TRUE    |      260617.00 |
|    4 |     26 | Thursday  | FALSE   |      340291.00 |
|    4 |     27 | Tuesday   | FALSE   |      319568.00 |
|    4 |     28 | Wednesday | FALSE   |      434460.00 |
|    5 |     29 | Friday    | FALSE   |      620860.00 |
|    5 |     30 | Monday    | FALSE   |      389080.00 |
|    5 |     31 | Saturday  | TRUE    |        1440.00 |
|    5 |     32 | Sunday    | TRUE    |      138421.00 |
|    5 |     33 | Thursday  | FALSE   |      549658.00 |
|    5 |     34 | Tuesday   | FALSE   |      367824.00 |
|    5 |     35 | Wednesday | FALSE   |      445366.00 |

The table above shows the totals for each day. From first glance, we see
that the total activity is higher on weekends compared to the weekdays.
We can also see that as the weeks progress, activity levels appear to
slightly decrease.

### Plotting activity through the day

Accelerometer data allows the inspection activity over the course of the
day. Using `summarise`, we can get the mean activity counts for each
minute of the day, for each day of the week.

``` r
daily_activity = accel_data %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    values_to = "activity",
    names_prefix = "activity_") %>% 
  mutate(minute = as.numeric(minute)) %>% 
  group_by(day, minute) %>% 
  summarise(mean_activity = mean(activity))
```

Based on this data, we can plot the mean 24-hour activity time course
for each day of the week.

``` r
activity_plot = daily_activity %>% 
  ggplot(aes(x = minute, y = mean_activity, color = day)) + 
  geom_point(alpha = 0.1, size = 0.8) + 
  geom_smooth(se = FALSE) + 
  labs(
    x = "Minute",
    y = "Mean activity",
    title = "Mean daily accelerometer activity by day of the week",
    caption = "Data come from the accel_data.csv dataset",
    color = "Day"
  ) + 
  scale_x_continuous(limits = c(0, 1400)) + 
  scale_y_continuous(limits = c(0, 2300)) + 
  viridis::scale_color_viridis(discrete = TRUE)

activity_plot
```

![](p8105_hw3_my2731_files/figure-gfm/activity_plot-1.png)<!-- -->

Based on the graph, we can see that daily activity is lowest in the
first 250 minutes of the day, and begins to increase and peak by
mid-day. This is reflective of the individual being asleep from midnight
until the morning. There is also a second peak of activity in the latter
quarter of the day. The days of the week with the most activity appear
to be on Friday evening and night, and Sunday mid-day.

## Problem 3: NOAA Data

### Clean and load data

``` r
data("ny_noaa")
ny_noaa
```

    ## # A tibble: 2,595,176 × 7
    ##    id          date        prcp  snow  snwd tmax  tmin 
    ##    <chr>       <date>     <int> <int> <int> <chr> <chr>
    ##  1 US1NYAB0001 2007-11-01    NA    NA    NA <NA>  <NA> 
    ##  2 US1NYAB0001 2007-11-02    NA    NA    NA <NA>  <NA> 
    ##  3 US1NYAB0001 2007-11-03    NA    NA    NA <NA>  <NA> 
    ##  4 US1NYAB0001 2007-11-04    NA    NA    NA <NA>  <NA> 
    ##  5 US1NYAB0001 2007-11-05    NA    NA    NA <NA>  <NA> 
    ##  6 US1NYAB0001 2007-11-06    NA    NA    NA <NA>  <NA> 
    ##  7 US1NYAB0001 2007-11-07    NA    NA    NA <NA>  <NA> 
    ##  8 US1NYAB0001 2007-11-08    NA    NA    NA <NA>  <NA> 
    ##  9 US1NYAB0001 2007-11-09    NA    NA    NA <NA>  <NA> 
    ## 10 US1NYAB0001 2007-11-10    NA    NA    NA <NA>  <NA> 
    ## # … with 2,595,166 more rows

The `ny_noaa` dataset contains weather data from NY state weather
stations between January 1, 1981 and December 31, 2010. It contains
2595176 observations of 7 variables. Key identifiers include the `id` of
the station name and `date`. The weather data includes the amount of
precipitation (in tenths of mm), snowfall (in mm), snow depth (mm), and
max and min temperatures (in tenths of C). We can note significant
amounts of missing data:

``` r
missing_noaa = ny_noaa %>% 
  summarise(
    across(prcp:tmin, list(nmiss = ~ sum(is.na(.x)))))

knitr::kable(missing_noaa)
```

| prcp_nmiss | snow_nmiss | snwd_nmiss | tmax_nmiss | tmin_nmiss |
|-----------:|-----------:|-----------:|-----------:|-----------:|
|     145838 |     381221 |     591786 |    1134358 |    1134420 |

Under each column, we can see that a significant proportion of weather
data points are missing, especially for `tmax` and `tmin`, where \~43.7%
of the observations are missing.

Next, we will clean the data. We will separate variables for year,
month, and day, and we will convert the units of `prcp`, `tmax`, and
`tmin` from tenths of a unit to mm and Celsius, respectively.

``` r
ny_noaa = ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(
    year = as.numeric(year),
    month = month.name[as.numeric(month)],
    day = as.numeric(day),
    prcp = prcp*0.1,
    tmax = as.numeric(tmax)*0.1,
    tmin = as.numeric(tmin)*0.1
  )

ny_noaa 
```

    ## # A tibble: 2,595,176 × 9
    ##    id           year month      day  prcp  snow  snwd  tmax  tmin
    ##    <chr>       <dbl> <chr>    <dbl> <dbl> <int> <int> <dbl> <dbl>
    ##  1 US1NYAB0001  2007 November     1    NA    NA    NA    NA    NA
    ##  2 US1NYAB0001  2007 November     2    NA    NA    NA    NA    NA
    ##  3 US1NYAB0001  2007 November     3    NA    NA    NA    NA    NA
    ##  4 US1NYAB0001  2007 November     4    NA    NA    NA    NA    NA
    ##  5 US1NYAB0001  2007 November     5    NA    NA    NA    NA    NA
    ##  6 US1NYAB0001  2007 November     6    NA    NA    NA    NA    NA
    ##  7 US1NYAB0001  2007 November     7    NA    NA    NA    NA    NA
    ##  8 US1NYAB0001  2007 November     8    NA    NA    NA    NA    NA
    ##  9 US1NYAB0001  2007 November     9    NA    NA    NA    NA    NA
    ## 10 US1NYAB0001  2007 November    10    NA    NA    NA    NA    NA
    ## # … with 2,595,166 more rows

``` r
ny_noaa %>% 
  group_by(snow) %>% 
  summarise(n_obs = n()) %>% 
  arrange(desc(n_obs)) %>% 
  head(5) %>% 
  rename("Snowfall (mm)" = snow,
         "# observations" = n_obs) %>% 
  knitr::kable()
```

| Snowfall (mm) | \# observations |
|--------------:|----------------:|
|             0 |         2008508 |
|            NA |          381221 |
|            25 |           31022 |
|            13 |           23095 |
|            51 |           18274 |

For snowfall, the most commonly observed values are `0`, `NA`, and `25`,
respectively. Intuitively, this is reasonable because snow does not fall
year-round and therefore, most days of the year would observe 0mm of
snowfall.

### Plotting average max temperature

We will create a two-panel plot showing the average max temperature in
January and in July in each station across years.

``` r
janjul_p = ny_noaa %>% 
  group_by(month, year, id) %>% 
  summarise(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  filter(month == c("January", "July")) %>% 
  ggplot(aes(x = year, y = mean_tmax, colour = id)) + 
  geom_line(alpha = 0.3) + 
  labs(
    x = "Year",
    y = "Average daily maximum temperature (°C)",
    title = "Average daily maximum temperature in New York State, 1981 to 2010",
    caption = "Data come from the rnoaa package"
  ) + 
  viridis::scale_color_viridis(discrete = TRUE) + 
  theme(legend.key.height = unit(0.05, "in"),
        legend.key.width = unit(0.1, "in"), 
        legend.text = element_text(size = 4)) + 
  guides(col = guide_legend(ncol = 25)) +
  facet_grid(. ~ month, scales = "free") 

janjul_p
```

![](p8105_hw3_my2731_files/figure-gfm/jan%20and%20july%20temp%20plots-1.png)<!-- -->

The resulting plots show the average maximum daily temperature in
January and July from 1981-2010. The average max temperature in Jaunary
tends to fluctuate around 0 degrees Celsius, while the temperature for
July varies around 27 degrees Celsius. In addition, the average maximum
temperature for July appears to have more variation compared to that of
January.

### Plotting min vs. max temperature and snowfall

Next, we will make a two-panel plot showing (i) tmax vs tmin for the
full dataset; and (ii) a plot showing the distribution of snowfall
values greater than 0 and less than 100 separately by year.

``` r
tmax_tmin_p = ny_noaa %>% 
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_hex() + 
  labs(
    x = "Minimum temperature (°C)",
    y = "Maximum temperature (°C)",
    title = "Density of daily minimum vs. maximum temperature (°C)",
  ) + 
  theme(legend.key.width = unit(0.5, "in"),
        plot.title = element_text(size = 11)) +
  viridis::scale_fill_viridis(name = "", option = "plasma")

snowfall_p = ny_noaa %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = snow, colour = as.factor(year))) + 
  geom_density(alpha = 0.1) +
  labs(
    x = "Snowfall (mm)",
    y = "Density",
    title = "Density of daily snowfall (mm)",
    colour = "Year",
  ) + 
  theme(legend.text = element_text(size = 6), 
        legend.key.size = unit(0.15, "in"),
        plot.title = element_text(size = 11)) + 
  guides(col = guide_legend(ncol = 6)) +
  viridis::scale_colour_viridis(discrete = TRUE, option = "plasma")

weather_plots = tmax_tmin_p + snowfall_p

wrap_elements(weather_plots) + 
  ggtitle("Density distributions of min vs. max temperature and snowfall in New York State, 1981-2010") +
  labs(caption = "Data come from the rnoaa package")
```

![](p8105_hw3_my2731_files/figure-gfm/temp%20and%20snowfall%20plots-1.png)<!-- -->

The result is density plot of snowfall (mm) from 1981 to 2010, for
observations with snowfall greater than 0mm and less than 100mm, and a
hexagon density plot of the daily minimum and maximum temperature for
all observations.

For the snowfall plot, we can see that the height of the peaks are
decreasing year over year, indicative of rising temperatures which are
resulting in less snowfall. Meanwhile, for the temperature hexagon
density plot, we can see that low daily minimum temperatures correspond
to lower daily maximum temperatures, and the same goes for high
temperatures. There appears to be outliers where a minimum temperature
of approximately -30°C corresponds to a maximum temperature of 60°C, and
where minimum and maximum daily temperature both sit at 60°C. These
outliers may be indicative of measurement error and should be verified
to ensure they are valid data points.
