---
title: "P8105 Homework 3"
output: github_document
date: "October 15th, 2022"
---

```{r setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)
knitr::opts_chunk$set(
  warning = FALSE, 
  message = FALSE,
  fig.dim = c(12, 7))
```

## Problem 1: Instacart Data

We will be working with the “The Instacart Online Grocery Shopping Dataset 2017”. 

### Data overview and description 
```{r dataload}
data("instacart")

instacart
```
The dataset contains information about online grocery orders made through Instacart. There 1,384,617 observations of 15 variables, where each row in the dataset is a product from an order. Each order is associated with a unique order ID number and order ID. The variables `order_hour_of_day` and `order_dow` describe the time (hour of day and day of the week) orders were made. Each row also contains information about each product, such as `product_name`, `aisle`, and `department` of the product. The variable `reordered` indicates whether a product has been purchased by a user in the past, and `add_to_cart_order` indicates the order an item was placed in the cart.

For example, for `order_id == 1`, we can see the order was made at 10AM on a Thursday, 4 out of the 8 products are reorders, and that most of the products came from the produce and dairy eggs department.

## Data exploration

Now, we want to conduct some basic EDA and answer some questions about the `instacart` data.

1. Number of aisles

Using `group_by`, and `summarise`, we can determine the number of aisles, and the most popular aisles customers order from in the dataset.

```{r}
aisles = instacart %>% 
  group_by(aisle_id, aisle) %>% 
  summarise(
    n_obs = n()
  ) %>% 
  arrange(desc(n_obs))

aisles
```

There are `r nrow(aisles)` different aisles. The most popular aisles customers order from are fresh vegetables, fresh fruits, and packaged vegetables fruits, respectively.

2. Plotting number of items ordered in each aisle

```{r plot_aisles}
plot_aisles = aisles %>% 
  filter(n_obs >= 10000) %>% 
  ggplot(aes(x = reorder(aisle, n_obs), y = n_obs)) +
  geom_bar(stat = "identity") + 
  coord_flip() +
  labs(
    title = "Number of Instacart items ordered by aisle",
    x = "Aisle",
    y = "Number of items ordered",
    caption = "Data from The Instacart Online Grocery Shopping Dataset 2017 in the p8105.datasets package"
  )

plot_aisles
```

Through the bar plot, we can confirm that for aisles with over 10,000 items ordered, the most popular aisles are fresh fruits and vegetables, and the least popular aisles are butter, oils and vinegars, and dry pasta.

3. Popular items 

Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

```{r pop_items}
pop_items = instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarise(n_obs = n()) %>% 
  arrange(desc(n_obs),.by_group = TRUE) %>% 
  slice(1)

knitr::kable(pop_items)
```


4. Mean ordering times

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

## Problem 2: Accelerometer Data

### Clean and load data

In this problem, we will load, clean, and tidy the `accel_data.csv` dataset, which contains five weeks of accelerometer data collected on a 63 year-old male with BMI 25, admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). 

We will apply snake case to all the variables, categorize and re-level the `day` variable according to the days of the week, and add a logical `weekend` variable that is `TRUE` if `day == c("Saturday", "Sunday")` and `FALSE` otherwise. 

```{r accel_data}
accel_data =
  read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    weekend = ifelse(day == "Saturday" | day == "Sunday", TRUE, FALSE),
    day = factor(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", 
                                 "Thursday", "Friday", "Saturday"))) %>% 
  relocate(weekend, .after = day)

accel_data
```

The dataset contains 35 observations of 1444 variables, where each row describes the activity counts for all minutes of each day. The weekday of observation is indicated by `day`, and has a unique `day_id`. `week` indicates the week of observation, and the `weekend` indicates whether observation falls on a weekend. The variables `activity_*` are the activity counts for all 1440 minutes of a 24-hour day, starting at midnight.

### Aggregate across minutes

Using `rowSums` and `across` within the `mutate` function, we can compute the sum across minutes for each day to get the total activity over the day. 

```{r aggregate}
accel_data = accel_data %>%
  mutate(total_activity = rowSums(across(activity_1:activity_1440))) %>% 
  relocate(total_activity, .after = weekend) 

accel_data %>% 
  select(week:total_activity) %>% 
  knitr::kable()
```

The table above shows the totals for each day. From first glance, we see that the total activity is higher on weekends compared to the weekdays We can also see that as time progresses, activity levels appear to slightly decrease.

### Plotting activity through the day

Accelerometer data allows the inspection activity over the course of the day. Using `summarise`, we can get the mean activity counts for each minute of the day, for each day of the week. 

```{r daily_data}
daily_activity = accel_data %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    values_to = "activity",
    names_prefix = "activity_") %>% 
  mutate(minute = as.numeric(minute)) %>% 
  group_by(day, minute) %>% 
  summarise(mean_activity = mean(activity))

```

Based on this data, we can plot the mean 24-hour activity time course, according to the day of the week.

```{r activity_plot}
activity_plot = daily_activity %>% 
  ggplot(aes(x = minute, y = mean_activity, color = day)) + 
  geom_point(alpha = 0.1) + 
  geom_smooth(se = FALSE) + 
  labs(
    x = "Minute",
    y = "Mean activity",
    title = "Mean daily accelerometer activity by day of the week",
    caption = "Data come from the accel_data.csv dataset",
    color = "Day"
  ) + 
  scale_x_continuous(limits = c(0, 1400)) + 
  scale_y_continuous(limits = c(0, 2300)) + 
  viridis::scale_color_viridis(discrete = TRUE)

activity_plot
```

Based on the graph, we can see that daily activity is lowest in the first 250 minutes of the day, and begins to increase and peak by mid-day. This is reflective of the individual being asleep from midnight until the morning. There is also a second peak of activity in the latter quarter of the day. The days of the week with the most activity appear to be on Friday evening and night, and Sunday mid-day. 

## Problem 3: NOAA Data

### Clean and load data

```{r}
data("ny_noaa")

ny_noaa
```
The `ny_noaa` dataset contains weather data from NY state weather stations between 1981 and 2010. It contains 2 595 176 observations of 7 variables. Key identifiers include the `id` of the station name and `date`. The weather data includes the amount of precipitation (in tenths of mm), snowfall (in mm), snow depth (mm), and max and min temperatures (in tenths of C). We can note significant amounts of missing data:

```{r nmiss}
ny_noaa %>% 
  summarise(
    across(prcp:tmin, list(nmiss = ~ sum(is.na(.x)))))
```
Under each column, we can see that a significant proportion of weather data points are missing, especially for `tmax` and `tmin`, where almost half of the observations have missing values.

```{r noaa_clean}
noaa_clean = ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(
    year = as.numeric(year),
    month = month.name[as.numeric(month)],
    day = as.numeric(day))
```


Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

